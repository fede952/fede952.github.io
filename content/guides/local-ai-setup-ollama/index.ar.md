---
title: "توقف عن الدفع مقابل الذكاء الاصطناعي: شغّل DeepSeek و Llama 3 محلياً ومجاناً"
date: 2026-02-02
description: "تعلّم كيف تشغّل نماذج ذكاء اصطناعي قوية مثل DeepSeek و Llama 3 على حاسوبك مجاناً باستخدام Ollama. خصوصية كاملة، بدون رسوم شهرية، يعمل بدون إنترنت."
tags: ["AI", "Ollama", "Privacy", "Tutorial", "LocalLLM"]
categories: ["Guides", "Artificial Intelligence"]
author: "Federico Sella"
draft: false
---

لا تحتاج إلى اشتراك بقيمة 20$/شهرياً لاستخدام مساعد ذكاء اصطناعي قوي. باستخدام أداة مجانية ومفتوحة المصدر تُسمى **Ollama**، يمكنك تشغيل نماذج لغوية كبيرة متطورة — بما في ذلك **Llama 3 من Meta** و **DeepSeek-R1** — مباشرة على حاسوبك. بدون سحابة. بدون حساب. لا تغادر بياناتك جهازك أبداً.

يرشدك هذا الدليل خلال عملية الإعداد الكاملة في أقل من 10 دقائق.

## لماذا تشغّل الذكاء الاصطناعي محلياً؟

### خصوصية كاملة

عندما تستخدم خدمة ذكاء اصطناعي سحابية، كل أمر تكتبه يُرسل إلى خادم بعيد. يشمل ذلك مقتطفات الكود وأفكار الأعمال والأسئلة الشخصية — كل شيء. مع **نموذج لغوي محلي**، تبقى محادثاتك على جهازك. نقطة.

### بدون رسوم شهرية

ChatGPT Plus يكلف 20$/شهرياً. Claude Pro يكلف 20$/شهرياً. GitHub Copilot يكلف 10$/شهرياً. النموذج المحلي لا يكلف **شيئاً** بعد التحميل الأولي. النماذج مفتوحة المصدر ومجانية الاستخدام.

### يعمل بدون إنترنت

في طائرة؟ في كوخ بدون Wi-Fi؟ لا يهم. النموذج المحلي يعمل بالكامل على المعالج والذاكرة — لا حاجة لاتصال بالإنترنت.

---

## المتطلبات المسبقة

لا تحتاج إلى بطاقة رسومات أو محطة عمل متطورة. إليك الحد الأدنى:

- **نظام التشغيل:** Windows 10/11 أو macOS 12+ أو Linux
- **الذاكرة:** 8 جيجابايت كحد أدنى (16 جيجابايت موصى بها للنماذج الأكبر)
- **مساحة القرص:** حوالي 5 جيجابايت متاحة للتطبيق ونموذج واحد
- **اختياري:** بطاقة رسومات مخصصة (NVIDIA/AMD) تُسرّع الاستدلال لكنها **ليست مطلوبة**

---

## الخطوة 1: تحميل وتثبيت Ollama

**Ollama** هو بيئة تشغيل خفيفة تقوم بتحميل وإدارة وتشغيل النماذج اللغوية الكبيرة بأمر واحد. التثبيت بسيط على كل المنصات.

### Windows

1. قم بزيارة [ollama.com](https://ollama.com) وانقر على **Download for Windows**.
2. شغّل المثبّت — يستغرق حوالي دقيقة.
3. يعمل Ollama في الخلفية تلقائياً بعد التثبيت.

### macOS

لديك خياران:

```bash
# الخيار أ: Homebrew (موصى به)
brew install ollama

# الخيار ب: التحميل المباشر
# قم بزيارة https://ollama.com وحمّل ملف .dmg
```

### Linux

أمر واحد يتولى كل شيء:

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

بعد التثبيت، تأكد من أنه يعمل:

```bash
ollama --version
```

يجب أن ترى رقم الإصدار في الطرفية.

---

## الخطوة 2: شغّل نموذجك الأول — الأمر السحري

هذه هي اللحظة. افتح الطرفية واكتب:

```bash
ollama run llama3
```

هذا كل شيء. سيقوم Ollama بتحميل نموذج **Llama 3 8B** (حوالي 4.7 جيجابايت) عند التشغيل الأول، ثم يأخذك إلى جلسة دردشة تفاعلية مباشرة في الطرفية:

```
>>> من أنت؟
أنا Llama، نموذج لغوي كبير مدرّب بواسطة Meta.
كيف يمكنني مساعدتك اليوم؟

>>> اكتب دالة Python تتحقق ما إذا كان العدد أولياً.
def is_prime(n):
    if n < 2:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True
```

### جرّب DeepSeek-R1 لمهام الاستدلال

**DeepSeek-R1** يتفوق في الرياضيات والمنطق وحل المشكلات خطوة بخطوة:

```bash
ollama run deepseek-r1
```

### نماذج شائعة أخرى

| النموذج | الأمر | الأفضل لـ |
|---|---|---|
| Llama 3 8B | `ollama run llama3` | الدردشة العامة، البرمجة |
| DeepSeek-R1 8B | `ollama run deepseek-r1` | الرياضيات، المنطق، الاستدلال |
| Mistral 7B | `ollama run mistral` | سريع، متعدد الاستخدامات |
| Gemma 2 9B | `ollama run gemma2` | نموذج Google المفتوح |
| Qwen 2.5 7B | `ollama run qwen2.5` | المهام متعددة اللغات |

نفّذ `ollama list` لرؤية النماذج المحمّلة و `ollama rm <اسم_النموذج>` لحذف نموذج وتحرير مساحة القرص.

---

## الخطوة 3: أضف واجهة دردشة مع Open WebUI (اختياري)

الطرفية تعمل، لكن إذا أردت واجهة أنيقة **مشابهة لـ ChatGPT**، ثبّت **Open WebUI**. الطريقة الأسرع هي Docker:

```bash
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway \
  -v open-webui:/app/backend/data --name open-webui \
  --restart always ghcr.io/open-webui/open-webui:main
```

ثم افتح [http://localhost:3000](http://localhost:3000) في متصفحك. ستحصل على واجهة دردشة مألوفة مع سجل المحادثات وتبديل النماذج ورفع الملفات والمزيد — الكل متصل بنسخة Ollama المحلية.

> **بدون Docker؟** هناك واجهات خفيفة أخرى مثل [Chatbox](https://chatboxai.app) (تطبيق سطح المكتب) أو [Ollama Web UI](https://github.com/ollama-webui/ollama-webui) التي لا تحتاج Docker.

---

## الذكاء الاصطناعي المحلي مقابل السحابي: المقارنة الكاملة

| الميزة | ذكاء اصطناعي محلي (Ollama) | ذكاء اصطناعي سحابي (ChatGPT, Claude) |
|---|---|---|
| **الخصوصية** | بياناتك لا تغادر حاسوبك أبداً | البيانات تُرسل لخوادم بعيدة |
| **التكلفة** | مجاني بالكامل | 20$/شهرياً للمستويات المميزة |
| **الإنترنت مطلوب** | لا — يعمل بالكامل بدون اتصال | نعم — دائماً |
| **السرعة** | تعتمد على جهازك | سريع (معالجات رسومية في الخادم) |
| **جودة النموذج** | ممتازة (Llama 3, DeepSeek) | ممتازة (GPT-4o, Claude) |
| **جهد الإعداد** | أمر واحد | إنشاء حساب |
| **التخصيص** | تحكم كامل، ضبط دقيق | محدود |
| **الاحتفاظ بالبيانات** | أنت تتحكم بكل شيء | تطبّق سياسة المزود |

**الخلاصة:** النماذج السحابية لا تزال تتفوق في القدرة الخام للمهام الأكبر، لكن للمساعدة اليومية في البرمجة والكتابة والعصف الذهني والأسئلة، النماذج المحلية **أكثر من كافية** — وهي مجانية وخاصة.

---

## الخاتمة

تشغيل ذكاء اصطناعي محلي لم يعد هواية متخصصة للباحثين ذوي بطاقات الرسومات الغالية. بفضل **Ollama** ومنظومة النماذج مفتوحة المصدر، يمكن لأي شخص يملك حاسوباً محمولاً حديثاً الحصول على مساعد ذكاء اصطناعي خاص ومجاني وقابل للعمل بدون إنترنت في أقل من 10 دقائق.

الأوامر التي يجب تذكرها:

```bash
# التثبيت (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# تشغيل نموذج
ollama run llama3

# عرض قائمة النماذج
ollama list
```

جرّبه. بمجرد أن تختبر سرعة وخصوصية نموذج لغوي محلي، قد تجد نفسك تلجأ إلى السحابة أقل فأقل.

> تحتاج للتركيز أثناء البرمجة مع ذكائك الاصطناعي المحلي؟ جرّب [مازج الأصوات المحيطة ZenFocus ومؤقت بومودورو](/ar/tools/zen-focus/) — أداة أخرى تعمل بالكامل في متصفحك دون أي تتبع.
