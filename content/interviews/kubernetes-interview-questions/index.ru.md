---
title: "Kubernetes (K8s) Подготовка к собеседованию: Вопросы и ответы уровня Senior"
description: "20 продвинутых вопросов по Kubernetes для собеседований на позиции Senior DevOps и SRE. Охватывает архитектуру, жизненный цикл подов, сеть, хранилище, RBAC и устранение неполадок в продакшене."
date: 2026-02-11
tags: ["kubernetes", "interview", "devops", "cloud-native"]
keywords: ["k8s interview questions", "cka exam prep", "kubernetes architecture questions", "kubernetes interview answers", "pod lifecycle interview", "kubernetes networking questions", "kubernetes rbac", "senior sre interview", "kubernetes troubleshooting", "helm interview questions"]
draft: false
schema_json: >
  {
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "name": "Kubernetes (K8s) Подготовка к собеседованию: Вопросы и ответы уровня Senior",
    "description": "20 продвинутых вопросов по Kubernetes об архитектуре, сети, хранилище, безопасности и устранении неполадок в продакшене.",
    "proficiencyLevel": "Advanced",
    "inLanguage": "ru"
  }
---

## Инициализация системы

Kubernetes — это операционная система облака и самый востребованный навык для ролей DevOps, SRE и Platform Engineering. Собеседования уровня Senior идут в глубину: вас будут спрашивать о внутреннем устройстве control plane, сетевых моделях, RBAC, управлении ресурсами и о том, как отлаживать инциденты в продакшене под давлением. Это руководство содержит 20 вопросов, которые регулярно встречаются на собеседованиях в ведущих технологических компаниях, с ответами, демонстрирующими глубину, ожидаемую на уровне Staff/Senior.

**Нужно быстро освежить команды?** Держите наш [Kubernetes Kubectl Cheat Sheet](/cheatsheets/kubernetes-kubectl-cheat-sheet/) открытым во время подготовки.

---

## Архитектура

<details>
<summary><strong>1. Опишите компоненты control plane Kubernetes и их обязанности.</strong></summary>
<br>

Control plane управляет состоянием кластера:

- **kube-apiserver**: Входная точка кластера. Каждая команда `kubectl`, действие контроллера и решение планировщика проходят через API-сервер. Он валидирует и сохраняет состояние в etcd.
- **etcd**: Распределённое хранилище ключ-значение, содержащее всё состояние кластера (желаемое состояние, фактическое состояние, конфигурации, секреты). Это единственный источник истины.
- **kube-scheduler**: Отслеживает вновь созданные поды без назначенного узла и выбирает узел на основе требований к ресурсам, правил аффинности, taints и ограничений.
- **kube-controller-manager**: Запускает циклы контроллеров (контроллеры Deployment, ReplicaSet, Node, Job), которые непрерывно сверяют желаемое состояние с фактическим.
- **cloud-controller-manager**: Интегрируется с API облачного провайдера для LoadBalancer'ов, провизионирования хранилища и жизненного цикла узлов.
</details>

<details>
<summary><strong>2. Что происходит, когда вы выполняете `kubectl apply -f deployment.yaml`?</strong></summary>
<br>

1. `kubectl` отправляет HTTP POST/PATCH на **API-сервер** с манифестом Deployment.
2. API-сервер **валидирует** запрос (аутентификация, авторизация через RBAC, admission-контроллеры).
3. API-сервер записывает объект Deployment в **etcd**.
4. **Контроллер Deployment** обнаруживает новый Deployment и создаёт **ReplicaSet**.
5. **Контроллер ReplicaSet** обнаруживает его и создаёт указанное количество объектов **Pod**.
6. **Планировщик** обнаруживает нераспределённые поды и назначает каждый на узел на основе доступности ресурсов и ограничений.
7. **kubelet** на каждом назначенном узле обнаруживает назначение пода, скачивает образ контейнера и запускает контейнер через среду выполнения контейнеров (containerd/CRI-O).
8. **kube-proxy** на каждом узле обновляет правила iptables/IPVS, если связан Service.
</details>

<details>
<summary><strong>3. В чём разница между Deployment, StatefulSet и DaemonSet?</strong></summary>
<br>

- **Deployment**: Управляет stateless-приложениями. Поды взаимозаменяемы, могут масштабироваться свободно и создаются/удаляются в любом порядке. Лучше всего для веб-серверов, API, воркеров.
- **StatefulSet**: Управляет stateful-приложениями. Каждый под получает **стабильное имя хоста** (`pod-0`, `pod-1`), **постоянное хранилище** (PVC на под), и поды создаются/удаляются **по порядку**. Лучше всего для баз данных, Kafka, ZooKeeper.
- **DaemonSet**: Гарантирует **один под на узел**. Когда новый узел присоединяется к кластеру, под автоматически планируется на нём. Лучше всего для сборщиков логов, агентов мониторинга, сетевых плагинов.
</details>

<details>
<summary><strong>4. Объясните жизненный цикл пода и его фазы.</strong></summary>
<br>

Под проходит через эти фазы:

1. **Pending**: Под принят, но ещё не распределён, или образы загружаются.
2. **Running**: Как минимум один контейнер работает или запускается/перезапускается.
3. **Succeeded**: Все контейнеры завершились с кодом 0 (для Jobs/пакетных задач).
4. **Failed**: Все контейнеры завершились, как минимум один вышел с ненулевым кодом.
5. **Unknown**: Узел недоступен, состояние пода не может быть определено.

Внутри работающего пода контейнеры могут находиться в состояниях: **Waiting** (загрузка образа, init-контейнеры), **Running** или **Terminated** (завершился или упал).
</details>

## Сеть

<details>
<summary><strong>5. Объясните сетевую модель Kubernetes.</strong></summary>
<br>

Сеть Kubernetes следует трём фундаментальным правилам:

1. **Каждый под получает свой IP-адрес** — без NAT между подами.
2. **Все поды могут связываться со всеми другими подами** между узлами без NAT.
3. **IP, который под видит для себя** — это тот же IP, который другие используют для связи с ним.

Это реализуется плагинами CNI (Container Network Interface), такими как Calico, Flannel, Cilium или Weave. Они создают overlay- или underlay-сеть, удовлетворяющую этим правилам. Каждый узел получает подсеть CIDR для подов, а плагин CNI обеспечивает маршрутизацию между узлами.
</details>

<details>
<summary><strong>6. В чём разница между сервисами ClusterIP, NodePort и LoadBalancer?</strong></summary>
<br>

- **ClusterIP** (по умолчанию): Внутренний виртуальный IP. Доступен только изнутри кластера. Используется для межсервисного взаимодействия.
- **NodePort**: Экспонирует сервис на статическом порту (30000-32767) на IP каждого узла. Внешний трафик может достигать `<NodeIP>:<NodePort>`. Строится поверх ClusterIP.
- **LoadBalancer**: Провизионирует внешний балансировщик нагрузки через облачного провайдера. Получает публичный IP/DNS. Строится поверх NodePort. Используется для публичных сервисов в продакшене.

Также существует **ExternalName**, который сопоставляет сервис с DNS CNAME (без проксирования, только DNS-разрешение).
</details>

<details>
<summary><strong>7. Что такое Ingress и чем он отличается от Service?</strong></summary>
<br>

**Service** работает на уровне 4 (TCP/UDP) — маршрутизирует трафик к подам на основе IP и порта.

**Ingress** работает на уровне 7 (HTTP/HTTPS) — маршрутизирует трафик на основе имени хоста и пути URL. Один Ingress может маршрутизировать `api.example.com` к API-сервису и `app.example.com` к frontend-сервису, всё через один балансировщик нагрузки.

Ingress требует **Ingress Controller** (nginx-ingress, Traefik, HAProxy, AWS ALB) для реализации правил маршрутизации. Ресурс Ingress — это лишь конфигурация, контроллер выполняет работу.
</details>

<details>
<summary><strong>8. Как работает DNS внутри кластера Kubernetes?</strong></summary>
<br>

Kubernetes запускает **CoreDNS** (или kube-dns) как дополнение кластера. Каждый сервис получает DNS-запись:

- `<service-name>.<namespace>.svc.cluster.local` → ClusterIP
- `<pod-ip-dashed>.<namespace>.pod.cluster.local` → Pod IP

Когда под делает DNS-запрос для `my-service`, резолвер в `/etc/resolv.conf` (настроенный kubelet) добавляет поисковые домены и запрашивает CoreDNS. CoreDNS отслеживает API-сервер на предмет изменений Service/Endpoint и обновляет свои записи автоматически.
</details>

## Хранилище

<details>
<summary><strong>9. Объясните PersistentVolume (PV), PersistentVolumeClaim (PVC) и StorageClass.</strong></summary>
<br>

- **PersistentVolume (PV)**: Фрагмент хранилища, провизионированный администратором или динамически через StorageClass. Существует независимо от любого пода. Имеет жизненный цикл, отдельный от подов.
- **PersistentVolumeClaim (PVC)**: Запрос на хранилище от пода. Указывает размер, режим доступа и опционально StorageClass. Kubernetes привязывает PVC к подходящему PV.
- **StorageClass**: Определяет класс хранилища (SSD, HDD, NFS) и провизионер, который динамически создаёт PV. Обеспечивает провизионирование хранилища по требованию — без вмешательства администратора.

Процесс: Под ссылается на PVC → PVC запрашивает хранилище у StorageClass → StorageClass запускает провизионер → Провизионер создаёт PV → PVC привязывается к PV → Под монтирует PV.
</details>

<details>
<summary><strong>10. Что такое режимы доступа и политики освобождения?</strong></summary>
<br>

**Режимы доступа**:
- **ReadWriteOnce (RWO)**: Монтируется для чтения/записи одним узлом. Самый распространённый (AWS EBS, GCE PD).
- **ReadOnlyMany (ROX)**: Монтируется только для чтения многими узлами. Используется для общих конфигураций.
- **ReadWriteMany (RWX)**: Монтируется для чтения/записи многими узлами. Требует сетевого хранилища (NFS, EFS, CephFS).

**Политики освобождения** (что происходит при удалении PVC):
- **Retain**: PV сохраняется с данными. Администратор должен вручную освободить его.
- **Delete**: PV и базовое хранилище удаляются. По умолчанию для динамического провизионирования.
- **Recycle** (устарело): Базовый `rm -rf` на томе. Используйте Retain или Delete вместо этого.
</details>

## Безопасность и RBAC

<details>
<summary><strong>11. Как работает RBAC в Kubernetes?</strong></summary>
<br>

RBAC (управление доступом на основе ролей) имеет четыре объекта:

- **Role**: Определяет разрешения (глаголы: get, list, create, delete) на ресурсы (поды, сервисы, секреты) в рамках **одного namespace**.
- **ClusterRole**: То же, что Role, но **на уровне кластера** (все namespace или кластерные ресурсы, такие как узлы).
- **RoleBinding**: Привязывает Role к пользователю, группе или сервисному аккаунту в рамках namespace.
- **ClusterRoleBinding**: Привязывает ClusterRole к субъекту по всему кластеру.

Принцип: Начинайте с минимально необходимых разрешений. Никогда не привязывайте `cluster-admin` к сервисным аккаунтам приложений. Регулярно проверяйте RBAC с помощью `kubectl auth can-i`.
</details>

<details>
<summary><strong>12. Что такое Pod Security Standards (PSS)?</strong></summary>
<br>

Pod Security Standards заменили PodSecurityPolicies (удалены в K8s 1.25). Они определяют три уровня безопасности:

- **Privileged**: Без ограничений. Разрешает всё. Используется для системных подов (CNI-плагины, драйверы хранилища).
- **Baseline**: Предотвращает известные эскалации привилегий. Блокирует hostNetwork, hostPID, привилегированные контейнеры, но разрешает большинство рабочих нагрузок.
- **Restricted**: Максимальная безопасность. Требует non-root, сброс всех capabilities, файловая система root только для чтения, запрет эскалации привилегий.

Применяется через контроллер **Pod Security Admission** на уровне namespace с помощью меток:
```yaml
metadata:
  labels:
    pod-security.kubernetes.io/enforce: restricted
```
</details>

<details>
<summary><strong>13. Как безопасно управлять секретами в Kubernetes?</strong></summary>
<br>

Стандартные секреты Kubernetes **закодированы в base64, не зашифрованы**. Любой с доступом к API может их декодировать.

Шаги по укреплению:
1. **Включить шифрование в покое** в etcd (`EncryptionConfiguration` с AES-CBC или KMS-провайдером).
2. **Использовать внешние менеджеры секретов** (Vault, AWS Secrets Manager) с External Secrets Operator или CSI Secrets Store Driver.
3. **RBAC**: Ограничить `get`/`list` секретов только теми сервисными аккаунтами, которым они нужны.
4. **Монтировать как файлы**, а не переменные окружения — переменные окружения могут утечь через логи, дампы при сбоях и `/proc`.
5. **Ротировать секреты** регулярно и использовать кратковременные учётные данные, где возможно.
</details>

## Планирование и ресурсы

<details>
<summary><strong>14. Объясните запросы и лимиты ресурсов.</strong></summary>
<br>

- **Запросы (Requests)**: Объём CPU/памяти, **гарантированный** контейнеру. Планировщик использует запросы для определения, на каком узле достаточно ёмкости.
- **Лимиты (Limits)**: **Максимальный** объём, который контейнер может использовать. Если контейнер превышает лимит памяти, он убивается по OOM. При превышении лимита CPU он ограничивается.

Классы QoS на основе запросов/лимитов:
- **Guaranteed**: Запросы == Лимиты для всех контейнеров. Наивысший приоритет, последний для вытеснения.
- **Burstable**: Запросы < Лимиты. Средний приоритет.
- **BestEffort**: Без запросов и лимитов. Первый для вытеснения под давлением.

Лучшая практика: Всегда устанавливайте запросы (для точности планирования) и лимиты (для стабильности кластера).
</details>

<details>
<summary><strong>15. Что такое taints, tolerations и node affinity?</strong></summary>
<br>

- **Taints** применяются к узлам: "Не планируйте поды здесь, если они не толерируют этот taint." Пример: `kubectl taint nodes gpu-node gpu=true:NoSchedule`.
- **Tolerations** применяются к подам: "Я могу толерировать этот taint." Поды с подходящими tolerations могут быть запланированы на узлах с taints.
- **Node Affinity** — это спецификация пода, которая говорит "Предпочтительно или обязательно планировать на узлах с определёнными метками." Пример: требовать узлы с `disktype=ssd`.

Используйте вместе: Применить taint к GPU-узлам → только поды с GPU tolerations и GPU affinity попадут туда. Предотвращает трату дорогого оборудования не-GPU рабочими нагрузками.
</details>

## Устранение неполадок

<details>
<summary><strong>16. Под застрял в CrashLoopBackOff. Как вы его отлаживаете?</strong></summary>
<br>

`CrashLoopBackOff` означает, что контейнер продолжает падать, и Kubernetes ждёт перед перезапуском (экспоненциальная задержка до 5 минут).

Шаги отладки:
1. `kubectl describe pod <name>` — проверьте Events, Last State, Exit Code.
2. `kubectl logs <pod> --previous` — прочитайте логи упавшего экземпляра.
3. Анализ кода выхода: 1 = ошибка приложения, 137 = убит по OOM, 139 = segfault, 143 = SIGTERM.
4. Если контейнер падает слишком быстро для логов: `kubectl run debug --image=<image> --command -- sleep 3600` и выполните exec для инспекции окружения.
5. Проверьте, не настроены ли неправильно пробы readiness/liveness (проба попадает на неправильный порт/путь).
6. Проверьте лимиты ресурсов — контейнер может быть убит по OOM до того, как успеет что-либо залогировать.
</details>

<details>
<summary><strong>17. Service не маршрутизирует трафик к подам. Что вы проверяете?</strong></summary>
<br>

1. **Метки совпадают**: `spec.selector` Service должен точно совпадать с `metadata.labels` пода.
2. **Endpoints существуют**: `kubectl get endpoints <service>` — если пусто, селектор не совпадает ни с одним работающим подом.
3. **Поды в состоянии Ready**: Только поды, прошедшие пробы readiness, появляются в Endpoints. Проверьте `kubectl get pods` на статус Ready.
4. **Несоответствие портов**: `targetPort` Service должен совпадать с портом, на котором контейнер реально слушает.
5. **Network Policy**: NetworkPolicy может блокировать входящий трафик к подам.
6. **DNS**: Из отладочного пода выполните `nslookup <service-name>`, чтобы убедиться, что DNS-разрешение работает.
</details>

<details>
<summary><strong>18. Как выполнить развёртывание без простоя?</strong></summary>
<br>

1. **Стратегия rolling update** (по умолчанию): Установите `maxUnavailable: 0` и `maxSurge: 1`, чтобы старые поды удалялись только после того, как новые поды станут Ready.
2. **Пробы readiness**: Без пробы readiness Kubernetes считает под Ready сразу после запуска — трафик попадает на него до инициализации приложения.
3. **PreStop hook**: Добавьте lifecycle hook `preStop` с коротким sleep (5-10с), чтобы текущие запросы завершились до удаления пода из endpoints Service.
4. **PodDisruptionBudget (PDB)**: Гарантирует, что минимальное количество подов всегда доступно во время добровольных прерываний (drain узлов, обновления).
5. **Грациозное завершение**: Приложение должно обрабатывать SIGTERM и завершать активные запросы перед выходом.
</details>

<details>
<summary><strong>19. Что такое Horizontal Pod Autoscaler и как он работает?</strong></summary>
<br>

HPA автоматически масштабирует количество реплик подов на основе наблюдаемых метрик (CPU, память или пользовательские метрики).

Как это работает:
1. HPA опрашивает **Metrics Server** (или API пользовательских метрик) каждые 15 секунд.
2. Вычисляет: `desiredReplicas = ceil(currentReplicas × (currentMetric / targetMetric))`.
3. Если желаемые реплики отличаются от текущих, обновляет количество реплик Deployment.
4. Периоды охлаждения предотвращают колебания: стабилизация масштабирования вверх (по умолчанию 0с), стабилизация масштабирования вниз (по умолчанию 300с).

Требования: Metrics Server установлен, запросы ресурсов определены на контейнерах (для метрик CPU/памяти), настроены границы мин/макс реплик.
</details>

<details>
<summary><strong>20. В чём разница между liveness probe и readiness probe?</strong></summary>
<br>

- **Liveness probe**: "Жив ли контейнер?" Если проваливается, kubelet **убивает и перезапускает** контейнер. Используется для обнаружения дедлоков или зависших процессов.
- **Readiness probe**: "Готов ли контейнер обслуживать трафик?" Если проваливается, под **удаляется из endpoints Service** (трафик не направляется к нему), но контейнер НЕ перезапускается. Используется для периодов прогрева, проверок зависимостей, временной перегрузки.

Также существует **Startup probe**: Отключает liveness/readiness пробы до запуска приложения. Полезна для медленно запускающихся приложений для предотвращения преждевременного уничтожения.

Распространённая ошибка: Использование liveness probe, которая проверяет нижестоящую зависимость (базу данных). Если база данных падает, все поды перезапускаются — усугубляя сбой. Liveness должна проверять только само приложение.
</details>
